# -*- coding: utf-8 -*-
"""UNET architecture.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18QbkCuZMwfGji-SRc2iI4arwJ0QuYokc

# Training and testing a U-NET

The U-Net model is a simple fully  convolutional neural network that is used for binary segmentation i.e foreground and background pixel-wise classification. Mainly, it consists of two parts.

*   Contracting Path: we apply a series of conv layers and downsampling layers  (max-pooling) layers to reduce the spatial size
*   Expanding Path: we apply a series of upsampling layers to reconstruct the spatial size of the input.

The two parts are connected using a concatenation layers among different levels. This allows learning different features at different levels. At the end we have a simple conv 1x1 layer to reduce the number of channels to 1.

# Imports
"""

import numpy as np
import matplotlib.pyplot as plt
import os
from PIL import Image, ImageDraw
from tensorflow import keras
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Input, Conv2DTranspose, Concatenate, BatchNormalization, UpSampling2D
from tensorflow.keras.layers import  Dropout, Activation
from tensorflow.keras.optimizers import Adam, SGD
from tensorflow.keras.layers import LeakyReLU
from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping
from tensorflow.keras import backend as K
from tensorflow.keras.utils import plot_model
import tensorflow as tf
import glob
import random
import cv2
from random import shuffle
import time
import xmltodict

"""# Dataset

We will use the The Oxford-IIIT Pet Dataset. It contains 37 classes of dogs and cats with around 200 images per each class. The dataset contains labels as bounding boxes and segmentation masks. The total number of images in the dataset is a little more than 7K.

![alt text](http://www.robots.ox.ac.uk/~vgg/data/pets/pet_annotations.jpg)

Download the images/masks and unzip the files
"""

if not os.path.exists("images.tar.gz"):
  os.system('wget http://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz')
  os.system('wget http://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz')
  os.system('tar -xvzf images.tar.gz && tar -xvzf annotations.tar.gz')
  os.system('rm  images/*.mat')

"""Note that we have two folders. The first one is `images` which contains the raw images and annotation which contains the masks as a `binary` folder image.

# Generators
"""

def image_generator(files, batch_size = 32, sz = (256, 256)):

  if files is None:
      return None

  while True:

    # select a random batch
    batch = np.random.choice(files, size = batch_size)

    #variables for collecting batches of inputs and outputs
    batch_x = []
    batch_y = []

    for f in batch:
        # Get the bounding box
        with open(f'annotations/xmls/{f[:-4]}.xml') as file:
            info_xml = xmltodict.parse(file.read())
            if isinstance(info_xml['annotation']['object'], list):
                boxes = []
                for object in info_xml['annotation']['object']:
                    box_dict = object['bndbox']
                    boxes.append(((int(box_dict['xmin']), int(box_dict['ymin'])), (int(box_dict['xmax']), int(box_dict['ymax']))))
            else:
                box_dict = info_xml['annotation']['object']['bndbox']
                boxes = [((int(box_dict['xmin']), int(box_dict['ymin'])), (int(box_dict['xmax']), int(box_dict['ymax'])))]
        
        # Head mask

        #get the masks. Note that masks are png files
        mask = Image.open(f'annotations/trimaps/{f[:-4]}.png')
        head_mask = Image.fromarray(np.zeros((mask.size[1], mask.size[0]), dtype=np.uint8))
        draw = ImageDraw.Draw(head_mask)
        for box in boxes:
            draw.rectangle(box, fill=255) 
        background_image = Image.fromarray(np.ones((mask.size[1], mask.size[0]), dtype=np.uint8)*2)
        mask = Image.composite(mask, background_image, mask=head_mask)
        mask = np.array(mask.resize(sz))

        #preprocess the mask
        mask[mask >= 2] = 0
        mask[mask != 0 ] = 1

        batch_y.append(mask)

        #preprocess the raw images
        raw = Image.open(f'images/{f}')
        raw = raw.resize(sz)
        raw = np.array(raw)

        #check the number of channels because some of the images are RGBA or GRAY
        if len(raw.shape) == 2:
          raw = np.stack((raw,)*3, axis=-1)

        else:
          raw = raw[:,:,0:3]

        batch_x.append(raw)

    #preprocess a batch of images and masks
    batch_x = np.array(batch_x)/255.
    batch_y = np.array(batch_y)
    batch_y = np.expand_dims(batch_y,3)

    yield (batch_x, batch_y)

"""# IoU metric

The intersection over union (IoU) metric is a simple metric used to evaluate the performance of a segmentation algorithm. Given two masks $y_{true}, y_{pred}$ we evaluate

$$IoU = \\frac{y_{true} \\cap y_{pred}}{y_{true} \\cup y_{pred}}$$
"""

def mean_iou(y_true, y_pred):
    yt0 = K.cast(y_true[:,:,:,0], 'float32')
    yp0 = K.cast(y_pred[:,:,:,0] > 0.5, 'float32')
    inter = tf.math.count_nonzero(tf.logical_and(tf.equal(yt0, 1), tf.equal(yp0, 1)))
    union = tf.math.count_nonzero(tf.add(yt0, yp0))
    iou = tf.where(tf.equal(union, 0), 1., tf.cast(inter/union, 'float32'))
    return iou

"""# Model"""

def unet(sz = (256, 256, 3)):
    x = Input(sz)
    inputs = x

    #down sampling
    f = 8
    layers = []

    for i in range(0, 6):
        #x = Conv2D(f, 3, activation='relu', padding='same') (x)
        x = Conv2D(f, 3, padding='same') (x)
        x = LeakyReLU()(x)
        #x = Conv2D(f, 3, activation='relu', padding='same') (x)
        x = Conv2D(f, 3, padding='same') (x)
        x = LeakyReLU()(x)
        layers.append(x)
        #x = Conv2D(f, 1, strides=2, activation='relu', padding='same') (x)
        x = Conv2D(f, 1, strides=2, padding='same') (x)
        x = LeakyReLU()(x)
        f = f*2
    ff2 = 64

    #bottleneck
    j = len(layers) - 1
    #x = Conv2D(f, 3, activation='relu', padding='same') (x)
    x = Conv2D(f, 3, padding='same') (x)
    x = LeakyReLU()(x)
    #x = Conv2D(f, 3, activation='relu', padding='same') (x)
    x = Conv2D(f, 3, activation='relu', padding='same') (x)
    x = LeakyReLU()(x)
    x = Conv2DTranspose(ff2, 2, strides=(2, 2), padding='same') (x)
    x = Concatenate(axis=3)([x, layers[j]])
    j = j -1

    #upsampling
    for i in range(0, 5):
        ff2 = ff2//2
        f = f // 2
        #x = Conv2D(f, 3, activation='relu', padding='same') (x)
        x = Conv2D(f, 3, padding='same') (x)
        x = LeakyReLU()(x)
        #x = Conv2D(f, 3, activation='relu', padding='same') (x)
        x = Conv2D(f, 3, padding='same') (x)
        x = LeakyReLU()(x)
        x = Conv2DTranspose(ff2, 2, strides=(2, 2), padding='same') (x)
        x = Concatenate(axis=3)([x, layers[j]])
        j = j -1


    #classification
    #x = Conv2D(f, 3, activation='relu', padding='same') (x)
    x = Conv2D(f, 3, padding='same') (x)
    x = LeakyReLU()(x)
    #x = Conv2D(f, 3, activation='relu', padding='same') (x)
    x = Conv2D(f, 3, padding='same') (x)
    x = LeakyReLU()(x)
    outputs = Conv2D(1, 1, activation='sigmoid') (x)

    #model creation
    model = Model(inputs=[inputs], outputs=[outputs])
    model.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy', metrics = [mean_iou])

    model.summary()

    return model


"""# Callbacks

Simple functions to save the model at each epoch and show some predictions
"""

def build_callbacks():
    checkpointer = ModelCheckpoint(filepath='train.weights.h5', verbose=0, save_weights_only=True)
    callbacks = [checkpointer, PlotLearning()]
    return callbacks

# inheritance for training process plot
class PlotLearning(keras.callbacks.Callback):

    def __init__(self):
        self.store_dir = f'results_{time.time()}'
        os.makedirs(self.store_dir, exist_ok=True)

    def on_train_begin(self, logs={}):
        self.i = 0
        self.x = []
        self.losses = []
        self.val_losses = []
        self.acc = []
        self.val_acc = []
        #self.fig = plt.figure()
        self.logs = []

    def on_epoch_end(self, epoch, logs={}):
        self.logs.append(logs)
        self.x.append(self.i)
        self.losses.append(logs.get('loss'))
        self.val_losses.append(logs.get('val_loss'))
        self.acc.append(logs.get('mean_iou'))
        self.val_acc.append(logs.get('val_mean_iou'))
        self.i += 1
        res = f'i={self.i}, loss={logs.get("loss")}, val_loss={logs.get("val_loss")}, mean_iou={logs.get("mean_iou")}, val_mean_iou={logs.get("val_mean_iou")}'
        print(res)
        with open(f'{self.store_dir}/results.txt', 'a') as f:
            f.write(res + "\n")

        #choose a random test image and preprocess
        path = np.random.choice(all_files)
        raw = Image.open(f'images/{path}')
        raw = np.array(raw.resize((256, 256)))/255.
        if len(raw.shape) >= 3:
          raw = raw[:,:,0:3]
        else:
          raw = raw

        #predict the mask
        pred = model.predict(np.expand_dims(raw, 0))

        #mask post-processing
        msk  = pred.squeeze()
        msk = np.stack((msk,)*3, axis=-1)
        msk[msk >= 0.5] = 1
        msk[msk < 0.5] = 0

        #show the mask and the segmented image
        combined = np.concatenate([raw, msk, raw* msk], axis = 1)
        plt.axis('off')
        plt.imshow(combined)
        #plt.show()
        plt.savefig(f'{self.store_dir}/{self.i}.png')

"""# Training"""
batch_size = 32
all_images_files = os.listdir('annotations/xmls/')
all_images_files = list(map(lambda name: name[:-4], all_images_files))
all_files = os.listdir('images')
all_files = list(filter(lambda name: name[:-4] in all_images_files, all_files))
shuffle(all_files)

#split into training and testing
print("Training images:", len(all_files))

train_generator = image_generator(all_files, batch_size = batch_size)

x, y = next(train_generator)

plt.axis('off')
img = x[0]
msk = y[0].squeeze()
msk = np.stack((msk,)*3, axis=-1)

plt.imshow( np.concatenate([img, msk, img*msk], axis = 1))

model = unet()
train_steps = len(all_files) //batch_size
model.fit(train_generator,
                    epochs = 30, steps_per_epoch = train_steps, validation_data = None, validation_steps = 0,
                    callbacks = build_callbacks(), verbose = 0)

"""# Testing"""

if not os.path.exists('test.jpg'):
    os.system('wget http://r.ddmcdn.com/s_f/o_1/cx_462/cy_245/cw_1349/ch_1349/w_720/APL/uploads/2015/06/caturday-shutterstock_149320799.jpg -O test.jpg')

raw = Image.open('test.jpg')
raw = np.array(raw.resize((256, 256)))/255.
raw = raw[:,:,0:3]

#predict the mask
pred = model.predict(np.expand_dims(raw, 0))

#mask post-processing
msk  = pred.squeeze()
msk = np.stack((msk,)*3, axis=-1)
msk[msk >= 0.5] = 1
msk[msk < 0.5] = 0

#show the mask and the segmented image
combined = np.concatenate([raw, msk, raw* msk], axis = 1)
plt.axis('off')
plt.imshow(combined)
plt.savefig(f'result_test.png')

"""# References


1.   http://deeplearning.net/tutorial/unet.html
2.   https://github.com/ldenoue/keras-unet


"""
